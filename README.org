* TODO 项目说明
:properties:
:custom_id: 6264cdb993a3e7bc1dc9d7487dd2fe6f
:id: 6264cdb993a3e7bc1dc9d7487dd2fe6f
:date: 2023-09-26 11:14:31 周二
:end:

这个项目将开发目前的一些有关 Streaming perception 的想法，看看结果怎么样

整个项目的基础是 StreamYOLO，这个项目的特点是把所有模型的获取都放在 =./cfgs/<...>.py= 脚本里，其他的 =./tools/= 下的脚本都是在处理一些参数的

** 模型获取的过程
:properties:
:custom_id: 7b816de93f7f761f47334b1909fc5763
:id: 7b816de93f7f761f47334b1909fc5763
:date: 2023-09-28 11:09:39 周四
:end:

在项目中，实验的定义是在 =./cfgs/= 中给出的，其中一个脚本都只给出一个类，即 =Exp= ，在本项目的 =./tools/train_dil.py= 中可以看到这个类是如何调用的

这与系列的研究都是基于 yolox 的，这个项目有如下的工作方式

#+name: a0aaac14af2c0633454c7b4a619a6098
#+begin_src python
  args = make_parser().parse_args()        # 读取命令行参数
  exp = get_exp(args.exp_file, args.name)  # 加载模型
  main(exp, args)                          # 模型推理
#+end_src

这里的 =make_parser()= 需要自定义，返回一个由 =argparse= 包定义的参数解析器，不多介绍

exp 来自 =from yolox.exp import get_exp= ，是 yolox 包中定义的实验框架，我们可以通过重载这个类来完成不同的实验步骤，这个过程就是定义 =args.exp_file= 和 =args.name=

在改进代码的时候，最快速的做法不是读懂其中的每一行，而是把别人改进把地方找到，然后修改这些就可以了

我们可以看一下 =StreamYOLO/tools/train.py= 和 =DAMO-StreamNet/tools/train_dil.py= 的区别：

#+name: 620a29eade4f909a5f0b1a565cca424a
#+begin_src shell :results verbatim
  ssh lab_pc "diff ~/project/StreamYOLO/tools/train.py ~/project/DAMO-StreamNet/tools/train_dil.py"
#+end_src

#+RESULTS: 620a29eade4f909a5f0b1a565cca424a
: 47a48
: >     parser.add_argument("-t", "--teacher_ckpt", default=None, type=str, help="checkpoint file of teacher model")

很简单，改进的地方只有蒸馏这块，所以这个训练脚本可以不用看了

主要的修改部分可以从 =./cfgs/streamnet_s.py= 开始看

在 streamnet 的主模型结构中，有 long 分支和 short 分支，前者输入多帧的历史帧，后者输入当前帧

在 =./cfgs/streamnet_s.py= 的 =get_model= 函数中可以看到，两个分支分别是由 =DFPPAFPNLONGV3= 和 =DFPPAFPNSHORTV3= 定义的，它们两者的区别如下

#+name: 7d6d8245416aac2a247b0b1ed275a2f1
#+begin_src diff
16c16
< class DFPPAFPNLONGV3(nn.Module):
---
> class DFPPAFPNSHORTV3(nn.Module):
18c18
<     相比DFPPAFPNLONG，直接指定输出的通道数
---
>     相比DFPPAFPNSHORT，直接指定输出的通道数
31,32c31,32
<         # dynamic_fusion=False,
<         merge_form="pure_concat", # "pure_concat", "add"
---
>         # avg_channel=True, # 是否所有通道平均，False当前帧占一半通道
>         # dynamic_fusion=False, # 对除了当前帧的其它帧进一步融合，如果为True，则不使用aux layers
34d33
<
41d39
<         self.merge_form = merge_form
126,135c124,126
<             if self.merge_form == "pure_concat":
<                 pan_out2 = torch.cat(pan_out2s, dim=1) + rurrent_pan_out2
<                 pan_out1 = torch.cat(pan_out1s, dim=1) + rurrent_pan_out1
<                 pan_out0 = torch.cat(pan_out0s, dim=1) + rurrent_pan_out0
<             elif self.merge_form == "add":
<                 pan_out2 = torch.sum(torch.stack(pan_out2s), dim=0) + rurrent_pan_out2
<                 pan_out1 = torch.sum(torch.stack(pan_out1s), dim=0) + rurrent_pan_out1
<                 pan_out0 = torch.sum(torch.stack(pan_out0s), dim=0) + rurrent_pan_out0
<             else:
<                 raise Exception(f'merge_form must be in ["pure_concat", "add"].')
---
>             pan_out2 = torch.cat(pan_out2s, dim=1) + rurrent_pan_out2
>             pan_out1 = torch.cat(pan_out1s, dim=1) + rurrent_pan_out1
>             pan_out0 = torch.cat(pan_out0s, dim=1) + rurrent_pan_out0
137,146c128,131
<             if self.merge_form == "pure_concat":
<                 pan_out2 = torch.cat(pan_out2s, dim=1)
<                 pan_out1 = torch.cat(pan_out1s, dim=1)
<                 pan_out0 = torch.cat(pan_out0s, dim=1)
<             elif self.merge_form == "add":
<                 pan_out2 = torch.sum(torch.stack(pan_out2s), dim=0)
<                 pan_out1 = torch.sum(torch.stack(pan_out1s), dim=0)
<                 pan_out0 = torch.sum(torch.stack(pan_out0s), dim=0)
<             else:
<                 raise Exception(f'merge_form must be in ["pure_concat", "add"].')
---
>             pan_out2 = torch.cat(pan_out2s, dim=1)
>             pan_out1 = torch.cat(pan_out1s, dim=1)
>             pan_out0 = torch.cat(pan_out0s, dim=1)
>
147a133
>         rurrent_pan_outs = (rurrent_pan_out2, rurrent_pan_out1, rurrent_pan_out0)
149c135
<         return outputs
---
>         return outputs, rurrent_pan_outs
#+end_src

** 历史帧的动态过程的实现
:properties:
:custom_id: 04091d29d246529b6d5aa820ad9e6fe4
:id: 04091d29d246529b6d5aa820ad9e6fe4
:date: 2023-09-28 18:35:13 周四
:end:

在这里有个问题，历史帧的输入长度是固定的：

#+name: 37146243c5c0f35f438c6f133ed3265d
#+begin_src python
  # 输入当前帧的分支
  self.short_cfg = dict(
                      frame_num=1,  # short只接受一帧
                      delta=1,
                      with_short_cut=False,
                      out_channels=[((64, 128, 256), 1), ],
                  )
  # 输入历史帧的分支
  self.long_cfg = dict(
                      frame_num=3,  # long接受三帧
                      delta=1,
                      with_short_cut=False,
                      include_current_frame=False,
                      out_channels=[((21, 42, 85), 3), ],
                  )
#+end_src

选择的方式是直接定义一组历史帧处理分支：

#+name: 0c160c080d054ae006e34d5aa6394868
#+begin_src python
  self.long_cfg = [
      dict(
          frame_num=fn,
          delta=1,
          with_short_cut=False,
          include_current_frame=False,
          out_channels=[((21, 42, 85), 3), ],
      ) for fn in [1, 2, 3, 4] # 在这里定义需要纳入计算的历史帧
  ]
#+end_src

后面在 =get_model= 函数中，初始化一组特征处理模型：

#+name: a822d8f62d58ed15bccf5b8f4f2d1f6f
#+begin_src python
  long_backbone_s = [
      DFPPAFPNLONGV3(
          self.depth,
          self.width,
          in_channels=in_channels,
          frame_num=self.long_cfg[i]["frame_num"],
          with_short_cut=self.long_cfg[i]["with_short_cut"],
          out_channels=self.long_cfg[i]["out_channels"]
      ) for i in range(len(self.long_cfg))
  ]
#+end_src

之后，再定义一个速度检测器，它输入的是多个帧，输出是一个评分，说明当前的时刻是什么速度

这里就存在很多问题了，比如说，怎么约束？怎么训练各个分支？

** Model 的实现过程
:properties:
:custom_id: 201ce23e51aa377483933d6f0856d62c
:id: 201ce23e51aa377483933d6f0856d62c
:date: 2023-10-03 16:26:01 周二
:end:

*** YOLOX
:properties:
:custom_id: 0f123fbb4bd67a02d433556cff4bcbef
:id: 0f123fbb4bd67a02d433556cff4bcbef
:date: 2023-10-03 17:07:38 周二
:end:

模型在实现过程中参照的主结构实际上是 yolox，在 =./exps/model/yolox.py= 中定义了它的实际结构

#+name: b41f22a115c7eac9cdbb23d5e12ee986
#+begin_src python
  class YOLOX(nn.Module):
      def __init__(self, backbone=None, head=None):
          super().__init__()
          if backbone is None:
              backbone = DFPPAFPN() # 这里是默认的backbone
          if head is None:
              head = TALHead(20) # 这里是默认的head
          self.backbone = backbone
          self.head = head
          # 从这里可以看到，yolox的主结构是很简洁的，只有backbone和head两个部分

      def forward(self,
                  x,                 # 输入
                  targets=None,      # 标签
                  buffer=None,       # 缓冲区
                  mode='off_pipe',   # 模式，可以分为离线和在线两种
                  ):
          assert mode in {'off_pipe', 'on_pipe'}
          if mode == 'off_pipe': # 离线模式下的情况
              fpn_outs = self.backbone(x, buffer=buffer, mode=mode)
              if self.training:
                  assert targets is not None
                  # 离线模式+训练情况下，返回多个损失
                  loss, iou_loss, conf_loss, cls_loss, l1_loss, num_fg = self.head(fpn_outs, targets, x)
                  outputs = {
                      "total_loss": loss,
                      "iou_loss": iou_loss,
                      "l1_loss": l1_loss,
                      "conf_loss": conf_loss,
                      "cls_loss": cls_loss,
                      "num_fg": num_fg,
                  }
              else:
                  outputs = self.head(fpn_outs)
              return outputs
          elif mode == 'on_pipe': # 在线模式下的情况，很简单，就是一个backbone加一个head
              fpn_outs, buffer_ = self.backbone(x, buffer=buffer, mode=mode)
              outputs = self.head(fpn_outs)
              return outputs, buffer_
#+end_src

我们这里将 =forward= 函数中有关离线＋训练情况下的代码去掉，可以看到，离线情况下是不返回 =buffer_= 的，而在线情况下需要返回

这个 =buffer_= 是由 =backbone= 返回的， =mode= 的区别也是在这里出现了不同

*** Longshort path 的实现
:properties:
:custom_id: 27cf2827ace4b0bc4a326faabc0f5aa0
:id: 27cf2827ace4b0bc4a326faabc0f5aa0
:date: 2023-10-03 19:47:11 周二
:end:

在 DAMO-Streamnet 中，这里同时新增了蒸馏的过程和 longshortnet 中的多个历史帧的连接过程

#+CAPTION:
#+ATTR_ORG: :width 300
#+NAME: 94a9cfbd52bcbfc8a58e046494d8c556
[[file:./images/screenshot-LNOELtoB.png]]

上面是 Longshortnet 中的核心结构，这里的 short path 是当前帧的输入，而 long path 是多个历史帧的输入

在 DAMO-StreamNet 中是没有实现在线检测这块内容的，这从源码 =./exps/model/yolox_longshort_v3_dil.py= 中可以看到

#+name: 0425ecf9631269426589f18d2069c43d
#+begin_src python
  class YOLOXLONGSHORTV3DIL(nn.Module):
      # ...

      def forward(self, x, targets=None, buffer=None, mode='off_pipe'):
          # fpn output content features of [dark3, dark4, dark5]
          # ...
          elif mode == 'on_pipe':
              fpn_outs, buffer_ = self.backbone(x,  buffer=buffer, mode='on_pipe')
              outputs = self.head(fpn_outs)
              return outputs, buffer_
#+end_src

如果要实现同等的 longshort path 过程，在线模式下不能只用一个输入直接计算的

这里我们来看一下离线部分的代码是如何实现的，即 =YOLOXLONGSHORTV3DIL= 类的 =forward= 函数

#+name: 52f93185c59469d1ddd66d7b636aac15
#+begin_src python
  def forward(self, x, target=None, buffer=None, mode='off_pipe'):
      assert mode in {'off_pipe', 'on_pipe'}
      if mode == 'off_pipe': # 离线检测
          if self.training:
              ...
          else:
              ...

          if not self.with_short_cut:
              ...
          else:
              ...

          if self.training:
              ...
          else:
              outputs = self.head(fpn_outs)

          return outputs

      elif mode == 'on_pipe': # 在线检测
          fpn_outs, buffer_ = self.backbone(x,  buffer=buffer, mode=mode)
          outputs = self.head(fpn_outs)
          return outputs, buffer_
#+end_src

对比前面的[[id:0f123fbb4bd67a02d433556cff4bcbef][YOLOX]]类，这个类的 =off_pipe= 模式中的行为复杂多了，训练时和测试时的不同不只体现在 =head= 的计算上，在 =backbone= 的计算上也是不同的，下面把 =off_pipe= 下的内容放大来看一下

第一步，先是特征提取过程，在 DAMO-Streamnet 中，teacher 网络在非训练情况下是不需要的，所以存在计算过程的差异

#+name: 015753e92ff5ef2b14e2fe2e82dac2c2
#+begin_src python
  if self.training:
      short_fpn_outs, rurrent_pan_outs = self.short_backbone(
          x[0][:, :-3, ...], buffer=buffer, mode=mode, backbone_neck=self.backbone
      ) # short path
      fpn_outs_t = self.backbone_t(x[0][:, -3:, ...], buffer=buffer, mode=mode) # teacher网络的特征输出
  else:
      # 在非训练情况下，teacher网络就不需要了
      short_fpn_outs, rurrent_pan_outs = self.short_backbone(
          x[0], buffer=buffer, mode='off_pipe', backbone_neck=self.backbone
      ) # short path

  # 这里long path的计算不管是否训练都是一样的，可以提出来
  long_fpn_outs = self.long_backbone(
      x[1], buffer=buffer, mode=mode, backbone_neck=self.backbone
  ) if self.long_backbone is not None else None # long path
#+end_src

之后是特征融合，在融合的时候，存在几种不同的情况（and, concat, pure_concat 和 long_fusion）

#+name: 2902dc09320d298d6c0961ff732d6374
#+begin_src python
  if not self.with_short_cut:
      if self.long_backbone is None:
          fpn_outs = short_fpn_outs
      else:
          if self.merge_form == "add":
              fpn_outs = [x + y for x, y in zip(short_fpn_outs, long_fpn_outs)]
          elif self.merge_form == "concat":
              fpn_outs_2 = torch.cat([self.jian2(short_fpn_outs[0]), self.jian2(long_fpn_outs[0])], dim=1)
              fpn_outs_1 = torch.cat([self.jian1(short_fpn_outs[1]), self.jian1(long_fpn_outs[1])], dim=1)
              fpn_outs_0 = torch.cat([self.jian0(short_fpn_outs[2]), self.jian0(long_fpn_outs[2])], dim=1)
              fpn_outs = (fpn_outs_2, fpn_outs_1, fpn_outs_0)
          elif self.merge_form == "pure_concat":
              fpn_outs_2 = torch.cat([short_fpn_outs[0], long_fpn_outs[0]], dim=1)
              fpn_outs_1 = torch.cat([short_fpn_outs[1], long_fpn_outs[1]], dim=1)
              fpn_outs_0 = torch.cat([short_fpn_outs[2], long_fpn_outs[2]], dim=1)
              fpn_outs = (fpn_outs_2, fpn_outs_1, fpn_outs_0)
          elif self.merge_form == "long_fusion":
              fpn_outs_2 = torch.cat([short_fpn_outs[0], self.jian2(long_fpn_outs[0])], dim=1)
              fpn_outs_1 = torch.cat([short_fpn_outs[1], self.jian1(long_fpn_outs[1])], dim=1)
              fpn_outs_0 = torch.cat([short_fpn_outs[2], self.jian0(long_fpn_outs[2])], dim=1)
              fpn_outs = (fpn_outs_2, fpn_outs_1, fpn_outs_0)
          else:
              raise Exception(f"merge_form must be in ['add', 'concat', 'pure_concat', 'long_fusion']")
  else:
      if self.long_backbone is None:
          fpn_outs = [x + y for x, y in zip(short_fpn_outs, rurrent_pan_outs)]
      else:
          if self.merge_form == "add":
              fpn_outs = [x + y + z for x, y, z in zip(short_fpn_outs, long_fpn_outs, rurrent_pan_outs)]
          elif self.merge_form == "concat":
              fpn_outs_2 = torch.cat([self.jian2(short_fpn_outs[0]), self.jian2(long_fpn_outs[0])], dim=1)
              fpn_outs_1 = torch.cat([self.jian1(short_fpn_outs[1]), self.jian1(long_fpn_outs[1])], dim=1)
              fpn_outs_0 = torch.cat([self.jian0(short_fpn_outs[2]), self.jian0(long_fpn_outs[2])], dim=1)
              fpn_outs = (fpn_outs_2, fpn_outs_1, fpn_outs_0)
              fpn_outs = [x + y for x, y in zip(fpn_outs, rurrent_pan_outs)]
          elif self.merge_form == "pure_concat":
              fpn_outs_2 = torch.cat([short_fpn_outs[0], long_fpn_outs[0]], dim=1)
              fpn_outs_1 = torch.cat([short_fpn_outs[1], long_fpn_outs[1]], dim=1)
              fpn_outs_0 = torch.cat([short_fpn_outs[2], long_fpn_outs[2]], dim=1)
              fpn_outs = (fpn_outs_2, fpn_outs_1, fpn_outs_0)
              fpn_outs = [x + y for x, y in zip(fpn_outs, rurrent_pan_outs)]
          elif self.merge_form == "long_fusion":
              fpn_outs_2 = torch.cat([short_fpn_outs[0], self.jian2(long_fpn_outs[0])], dim=1)
              fpn_outs_1 = torch.cat([short_fpn_outs[1], self.jian1(long_fpn_outs[1])], dim=1)
              fpn_outs_0 = torch.cat([short_fpn_outs[2], self.jian0(long_fpn_outs[2])], dim=1)
              fpn_outs = (fpn_outs_2, fpn_outs_1, fpn_outs_0)
              fpn_outs = [x + y for x, y in zip(fpn_outs, rurrent_pan_outs)]
          else:
              raise Exception(f"merge_form must be in ['add', 'concat', 'pure_concat', 'long_fusion']")
#+end_src

最后，是 head 部分的计算

#+name: 077f9e2d7922f12146052a36f4266bea
#+begin_src python
  if self.training: # 在训练的时候，涉及到损失的计算，由于加入了蒸馏学习过程，损失项变多了
      assert targets is not None
      (loss, iou_loss, conf_loss, cls_loss, l1_loss, num_fg), reg_outputs, obj_outputs, cls_outputs = self.head(
           fpn_outs, targets, x
      )
      reg_outputs_t, obj_outputs_t, cls_outputs_t = self.head_t(fpn_outs_t)
      reg_dil_losses = []
      cls_dil_losses = []
      obj_dil_losses = []
      for i in range(len(reg_outputs)):
          cur_loss = self.dil_loss(reg_outputs[i], reg_outputs_t[i])
          reg_dil_losses.append(cur_loss)
          reg_dil_loss = self.reg_coef * (torch.sum(torch.stack(reg_dil_losses)) / self._get_tensors_numel(reg_outputs))
      for i in range(len(cls_outputs)):
          cur_loss = self.dil_loss(cls_outputs[i], cls_outputs_t[i])
          cls_dil_losses.append(cur_loss)
          cls_dil_loss = self.cls_coef * (torch.sum(torch.stack(cls_dil_losses)) / self._get_tensors_numel(cls_outputs))
      for i in range(len(obj_outputs)):
          cur_loss = self.dil_loss(obj_outputs[i], obj_outputs_t[i])
          obj_dil_losses.append(cur_loss)
          obj_dil_loss = self.obj_coef * (torch.sum(torch.stack(obj_dil_losses)) / self._get_tensors_numel(obj_outputs))

      dil_loss = self.dil_loss_coef * (reg_dil_loss + cls_dil_loss + obj_dil_loss)
      loss = self.det_loss_coef * loss
      total_loss = dil_loss + loss

      outputs = {
          "total_loss": total_loss,
          "det_loss": loss,
          "iou_loss": iou_loss,
          "l1_loss": l1_loss,
          "conf_loss": conf_loss,
          "cls_loss": cls_loss,
          "dil_loss": dil_loss,
          "reg_dil_loss": reg_dil_loss,
          "cls_dil_loss": cls_dil_loss,
          "obj_dil_loss": obj_dil_loss,
          "num_fg": num_fg,
      }
  else:
      outputs = self.head(fpn_outs) # 非训练的时候，直接计算并输出就可以了

  return outputs
#+end_src
